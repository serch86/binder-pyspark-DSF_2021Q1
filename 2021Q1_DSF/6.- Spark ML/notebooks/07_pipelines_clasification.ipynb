{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.5.2"}, "@webio": {"lastCommId": "5b3af32b012d4cf18500bf3c0eb2793e", "lastKernelId": "280ee430-1e8a-4ee5-b24f-9dda02bcdab5"}, "kernelspec": {"display_name": "d1-spark2python3", "language": "python", "name": "spark-python-d1-spark2python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": true, "toc_section_display": true, "base_numbering": 1, "toc_window_display": false, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {}, "nav_menu": {}}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["SANDBOX_NAME = ''# Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/\""]}, {"metadata": {}, "source": ["\n", "\n", "# Spark ML Pipelines\n", "\n", "Cargamos un dataset con informaci\u00f3n sobre cu\u00e1n seguro es un coche. Con este dataset se estudiar\u00e1n funciones muy importantes de Spark ML."], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "### Crear SparkSession\n", "\n", "Sabemos que en Datio no es necesario crear la sesi\u00f3n de Spark ya al iniciar un notebook con el Kernel PySpark Python3 - Spark 2.1.0 se crea autom\u00e1ticamente. Pero as\u00ed lo har\u00edamos si fuera necesario."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql import SparkSession\n", "\n", "spark = SparkSession.builder.getOrCreate()"]}, {"metadata": {}, "source": ["\n", "\n", "### Cargar datos y comprobar schema"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cars = spark.read.csv(DATA_PATH+'data/automobile.csv', sep=';', header=True, inferSchema=True)\n", "\n", "cars.printSchema()"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cars.show()"]}, {"metadata": {}, "source": ["\n", "\n", "### Vamos a trabajar con los valores nulos"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Una manera de devolver un dataframe sin filas que contengan nulos"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "Si quisieramos reemplazar los valores nulos con otro valor:"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "Otra forma de eliminar las filas con valores nulos (filtrar los nulos)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "Cambiamos la variable objetivo para hacerla binaria, para poder utilizar algoritmos de clasificaci\u00f3n binaria. \n", "* -2, -1, 0 => no es muy seguro (0)\n", "* 1, 2, 3 => s\u00ed es seguro (1)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Supongamos que queremos pasar la columna 'make' a dummy y luego lanzar un modelo de clasificaci\u00f3n.**\n", "\n", "**Resolviendo sin Pipeline** \n", "\n", "Tal como hemos visto antes, esto lo podr\u00edamos hacer paso a paso. \n", "\n", "1. Hacemos el StringIndexer a la columna _make_ para pasarla a num\u00e9rica (0... n_categorias-1)\n", "2. Hacemos el OneHotEncoder sobre el resultado del paso anterior para hacerla dummy\n", "3. Seleccionamos las variables que vamos a incluir en nuestro modelo (todas aquellas que no sean string y que no sean la variable objetivo _symboling_). Eso lo hacemos recorriendo `df.dtypes`, el cual nos devuelve una lista de tuplas, donde cada tupla tiene (nombre_variable, tipo_variable).\n", "4. Con las variables seleccionadas como predictoras del modelo, hacemos el VectorAssembler\n", "5. Dividimos train y test\n", "6. La salida del VectorAssembler ser\u00e1 lo que le demos al modelo (en este caso un Random Forest). Entrenamos (*fit*) y predecimos (*transform*)"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Resolviendo con Pipeline** \n", "\n", "Es parecido a lo que hicimos sin pipeline, pero en lugar de hacer:\n", "- Crear el objeto string indexer, hacer *fit*, hacer *transform*\n", "- Crear el objeto OHE, hacer *transform*\n", "- Etc. \n", "\n", "Lo que hacemos es simplemente crear los objetos, los metemos como *stages* del pipeline, y luego le hacemos *fit* y *transform* al pipeline. Vamos a verlo."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": [" \n", "\n", "Los pipelines nos permiten reproducir todo el flujo cada vez que tenemos nuevos datos, de esta manera nos aseguramos de que cada nuevo \"batch\" se somete exactamente al mismo procesado.\n", "\n", "**Para guardar el pipeline completo:**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": [" \n", "\n", "**Podemos despues cargar el pipeline de la siguiente manera**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "# Ejercicio 1\n", "\n", "Dado el siguiente DataFrame:"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql.functions import col as c\n", "from pyspark.sql import functions as F\n", "from pyspark.ml.feature import StandardScaler, MinMaxScaler\n", "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer, StringIndexerModel\n", "from pyspark.ml import Pipeline, PipelineModel\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "df = spark.read.csv(DATA_PATH + 'data/pokemon.csv', header=True, inferSchema=True)"]}, {"metadata": {}, "source": ["\n", "\n", "1) Realiza las transformaciones necesarias para comprobar que los datos son v\u00e1lidos para procesar (nombres de columnas, valores nulos).\n", "\n", "2) La variable que se va a predecir es una derivada de la columna `Speed`; cr\u00e9ala de forma que sea binaria, con 1 si el elemento supera la velocidad media del dataset, y 0 si no la supera.\n", "\n", "3) Define una funci\u00f3n que construya un `pipeline` con los pasos necesarios para preparar los datos y que aplique dicho pipeline a un DataFrame que reciba como par\u00e1metro de la funci\u00f3n.\n", "\n", "4) Construye un modelo de clasificaci\u00f3n usando `pyspark.ml.classification.RandomForestClassifier` para predecir si la velocidad de un pokemon es mayor a la media.\n", "\n", "5) Extrae la probabilidad de que la predicci\u00f3n sea 1 en una columna separada.\n", "\n", "6) Construye una funci\u00f3n que calcule los valores de *precision* y *recall* para diferentes valores de umbral (*threshold*), utilizando la definici\u00f3n (f\u00f3rmulas) de cada m\u00e9trica. Es decir, calcula el valor de las m\u00e9tricas para valores de umbral entre 0 y 1.\n", "\n", "**Extra**: Dibuja las curvas de *precision* y *recall* versus el umbral (eje x)."], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "**Parte 1**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 2**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 3**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 4**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 5**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 6**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}], "nbformat": 4, "nbformat_minor": 2}