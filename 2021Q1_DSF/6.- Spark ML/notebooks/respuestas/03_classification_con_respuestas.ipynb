{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.5.2"}, "@webio": {"lastCommId": "b87b22940b084b3a9710f55782d85548", "lastKernelId": "3dc96239-9553-4c8b-b9e8-2c6d468f96f5"}, "kernelspec": {"display_name": "d1-spark2python3", "language": "python", "name": "spark-python-d1-spark2python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": true, "toc_section_display": true, "base_numbering": 1, "toc_window_display": false, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {}, "nav_menu": {}}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["SANDBOX_NAME = '' # Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/\""]}, {"metadata": {}, "source": ["\n", "\n", "# Spark ML Clasificaci\u00f3n\n", "\n", "Cargamos un dataset observaciones diarias del tiempo desde varias estaciones meteorol\u00f3gicas australianas. La variable target RainTomorrow significa: \u00bfllover\u00e1 ma\u00f1ana? (Yes o No)"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "### Crear SparkSession"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql import SparkSession\n", "\n", "spark = SparkSession.builder.getOrCreate()"]}, {"metadata": {}, "source": ["\n", "\n", "### Cargar datos y comprobar schema (dataset inicial para ver las variables originales)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "weather = spark.read.csv(DATA_PATH+'data/weather_aus.csv', sep=',', header=True, inferSchema=True)\n", "\n", "weather.printSchema()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "weather.show()"]}, {"metadata": {}, "source": ["\n", "\n", "### Cargar datos y comprobar schema (dataset con variables dummies)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "weather = spark.read.csv(DATA_PATH+'data/weather_aus_prepared.csv', sep=',', header=True, inferSchema=True)\n", "\n", "weather.printSchema()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "weather.show(1)"]}, {"metadata": {}, "source": ["\n", "\n", "* Verificar valores nulos"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql import functions as F\n", "\n", "for column in weather.columns:\n", "    if weather.where(F.col(column).isNull()).count() != 0:\n", "        print(\"\\tBe careful: there are null values in the column '{}'\".format(column))\n", "    else:\n", "        print(\"The column '{}' does not have null values\".format(column))"]}, {"metadata": {}, "source": ["\n", "\n", "Nos disponemos a lanzar un algoritmo de clasificaci\u00f3n para predecir si llover\u00e1 o no ma\u00f1ana"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "#### Pasos previos\n", "\n", "* VectorAssembler con variables deseadas\n", "\n", "Se toman todas aquellas que son num\u00e9ricas menos la objetivo (en este caso es 'label')"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.feature import VectorAssembler\n", "\n", "variables_vector_assembler = [element for element in weather.columns if element != 'RainTomorrow']\n", "\n", "vector_assemmbler = VectorAssembler(inputCols = variables_vector_assembler, outputCol = 'assembled_features')\n", "\n", "weather = vector_assemmbler.transform(weather)\n", "\n", "weather.show()"]}, {"metadata": {}, "source": ["\n", "\n", "- Partir dataset entre train y test"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "weather_train, weather_test = weather.randomSplit([0.8,0.2])"]}, {"metadata": {}, "source": ["\n", "\n", "### Regresi\u00f3n Log\u00edstica"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.classification import LogisticRegression\n", "\n", "thld_label_1 = 0.45 # Try other values, like:  0.01 , 0.15, 0.30, 0.5 (default)\n", "logistic_regression = LogisticRegression(featuresCol= 'assembled_features', labelCol='RainTomorrow', threshold=thld_label_1 )\n", "print (\"Logistic regression threshold for 'RainTomorrow = 1.0' is: \",logistic_regression.getThreshold())\n", "\n", "logistic_regression_model = logistic_regression.fit(weather_train)\n", "print(\"Logistic regression coefficients: \" + str(logistic_regression_model.coefficientMatrix))\n", "print(\"Logistic regression intercept: \" + str(logistic_regression_model.interceptVector))\n", "\n", "weather_logistic_regression = logistic_regression_model.transform(weather_test)\n", "\n", "weather_logistic_regression.show(5, truncate=False)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["weather_logistic_regression.select(\"RainTomorrow\", \"rawPrediction\", \"probability\",\"prediction\").show()"]}, {"metadata": {}, "source": ["\n", "\n", "Lo primero que debemos entender es que el modelo regresi\u00f3n log\u00edstica de la librer\u00eda Spark, busca clasificar variables _target_ 'label = 1.0'. Esta clasificaci\u00f3n tiene un umbral de corte de probabilidad y est\u00e1 asociado con el par\u00e1metro _threshold_ (se recomienda probar distintos valores).\n", "\n", "Otra cosa importante del modelo  regresi\u00f3n log\u00edstica, es que cuando  se realiza para predicci\u00f3n con el m\u00e9todo *transform* se obtienen 3 columnas: *rawPrediction*, *probability*  y *prediction*\n", "\n", "A modo de ejemplo, se analiza un registro:\n", "\n", "| rawPrediction | probability | prediction |\n", "| :----------: | :----------: | :----------: |\n", "| [3.38, -3.38] | [0.96, 0.03]| 0.0|\n", "\n", "En la columna *rawPrediction* existen dos valores, el valor -3.38 sale de aplicar la siguiente formula: $\\beta X +\\beta_0$ Los valores en la columna *probability* salen de aplicar la funci\u00f3n sigmoide. Finalmente la columna de *prediction* aplica el _threshold=0.45_ sobre el segundo valor de probabilidad (es decir 0.03) y determinar que dicho registro debe ser clasificado como 0.0\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "### Random Forest"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "random_forest = RandomForestClassifier(featuresCol= 'assembled_features', labelCol='RainTomorrow',\n", "                                      maxDepth=8, numTrees=128, impurity=\"gini\")\n", "\n", "random_forest_model = random_forest.fit(weather_train)\n", "print('Learned classification random forest model:')\n", "print(\"\\t\",random_forest_model.getNumTrees)\n", "print(\"\\t\",random_forest_model.featureImportances)\n", "\n", "weather_random_forest = random_forest_model.transform(weather_test)\n", "\n", "weather_random_forest.show(5)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["weather_random_forest.select(\"RainTomorrow\", \"rawPrediction\", \"probability\",\"prediction\").show()"]}, {"metadata": {}, "source": ["\n", "\n", "### Gradient Boosting Trees"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.classification import GBTClassifier\n", "\n", "gbt = GBTClassifier(featuresCol= 'assembled_features', labelCol='RainTomorrow', maxIter=8, maxDepth=10, seed=1023)\n", "\n", "gbt_model = gbt.fit(weather_train)\n", "\n", "weather_gbt = gbt_model.transform(weather_test)\n", "\n", "weather_gbt.show(5)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["weather_gbt.select(\"RainTomorrow\", \"rawPrediction\", \"probability\",\"prediction\").show()"]}, {"metadata": {}, "source": ["\n", "# Evaluaci\u00f3n de los modelos"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Importamos las librerias necesarias para evaluar los modelos"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n", "from pyspark.mllib.evaluation import MulticlassMetrics"]}, {"metadata": {}, "source": ["\n", "\n", "Imprimir resultados para los distintos modelos"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "metrics = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='RainTomorrow')\n", "multimetrics = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='RainTomorrow')\n", "\n", "# In binary case this four metrics will return the same value\n", "accuracy = multimetrics.evaluate(weather_logistic_regression, {metrics.metricName: \"accuracy\"})\n", "recall = multimetrics.evaluate(weather_logistic_regression, {metrics.metricName: \"recall\"})\n", "precision = multimetrics.evaluate(weather_logistic_regression, {metrics.metricName: \"precision\"})\n", "f1 = multimetrics.evaluate(weather_logistic_regression, {metrics.metricName: \"f1\"})\n", "\n", "area_under_pr = metrics.evaluate(weather_logistic_regression, {metrics.metricName: \"areaUnderPR\"})\n", "area_under_roc = metrics.evaluate(weather_logistic_regression, {metrics.metricName: \"areaUnderROC\"})\n", "\n", "# We will call a function from mllib library. Therefore, we will be working with a RDD instead of working with a DataFrame\n", "metrics_rdd = MulticlassMetrics(weather_logistic_regression.select('prediction', 'RainTomorrow').rdd)\n", "confusion_matrix = metrics_rdd.confusionMatrix()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "print(\"Accuracy: {}\".format(accuracy))\n", "print(\"Recall: {}\".format(recall))\n", "print(\"Precision: {}\".format(precision))\n", "print(\"F1: {}\".format(f1))\n", "print(\"Area under PR: {}\".format(area_under_pr))\n", "print(\"Area under ROC: {}\".format(area_under_roc))\n", "print(\"Confusion matrix: {}\".format(confusion_matrix))"]}, {"metadata": {}, "source": ["\n", "\n", "Crea una funci\u00f3n que reciba como par\u00e1metro el nombre de la columna de predicci\u00f3n, la del target, y el dataframe, y devuelva un diccionario con todas las m\u00e9tricas del proyecto de clasificaci\u00f3n binaria."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "def calculate_metrics(prediction_column, model_dataframe):\n", "    metrics = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol=prediction_column)\n", "    multimetrics = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=prediction_column)\n", "\n", "    # In binary case this four metrics will return the same value\n", "    accuracy = multimetrics.evaluate(model_dataframe, {metrics.metricName: \"accuracy\"})\n", "    recall = multimetrics.evaluate(model_dataframe, {metrics.metricName: \"recall\"})\n", "    precision = multimetrics.evaluate(model_dataframe, {metrics.metricName: \"precision\"})\n", "    f1 = multimetrics.evaluate(model_dataframe, {metrics.metricName: \"f1\"})\n", "    \n", "    area_under_pr = metrics.evaluate(model_dataframe, {metrics.metricName: \"areaUnderPR\"})\n", "    area_under_roc = metrics.evaluate(model_dataframe, {metrics.metricName: \"areaUnderROC\"})\n", "\n", "    # We will call a function from mllib library. Therefore, we will be working with a RDD instead of working with a DataFrame\n", "    metrics_rdd = MulticlassMetrics(model_dataframe.select('prediction', prediction_column).rdd)\n", "    confusion_matrix = metrics_rdd.confusionMatrix()\n", "    \n", "    print(\"Accuracy: {}\".format(accuracy))\n", "    print(\"Recall: {}\".format(recall))\n", "    print(\"Precision: {}\".format(precision))\n", "    print(\"F1: {}\".format(f1))\n", "    print(\"Area under PR: {}\".format(area_under_pr))\n", "    print(\"Area under ROC: {}\".format(area_under_roc))\n", "    print(\"Confusion matrix: {}\".format(confusion_matrix))"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "models_dictionary = {\n", "    \"Logistic regression\": weather_logistic_regression,\n", "    \"Random Forest\": weather_random_forest,\n", "    \"GBT\": weather_gbt\n", "}\n", "\n", "prediction_column = \"RainTomorrow\"\n", "\n", "for k, v in models_dictionary.items():\n", "    print(k)\n", "    calculate_metrics(prediction_column, v)\n", "    print()"]}], "nbformat": 4, "nbformat_minor": 2}