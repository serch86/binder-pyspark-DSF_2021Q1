{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.5.2"}, "@webio": {"lastCommId": "5b3af32b012d4cf18500bf3c0eb2793e", "lastKernelId": "280ee430-1e8a-4ee5-b24f-9dda02bcdab5"}, "kernelspec": {"display_name": "d1-spark2python3", "language": "python", "name": "spark-python-d1-spark2python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": true, "toc_section_display": true, "base_numbering": 1, "toc_window_display": false, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {}, "nav_menu": {}}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["SANDBOX_NAME = ''# Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/\""]}, {"metadata": {}, "source": ["\n", "\n", "# Spark ML Pipelines\n", "\n", "Cargamos un dataset con informaci\u00f3n sobre cu\u00e1n seguro es un coche. Con este dataset se estudiar\u00e1n funciones muy importantes de Spark ML."], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "### Crear SparkSession\n", "\n", "Sabemos que en Datio no es necesario crear la sesi\u00f3n de Spark ya al iniciar un notebook con el Kernel PySpark Python3 - Spark 2.1.0 se crea autom\u00e1ticamente. Pero as\u00ed lo har\u00edamos si fuera necesario."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql import SparkSession\n", "\n", "spark = SparkSession.builder.getOrCreate()"]}, {"metadata": {}, "source": ["\n", "\n", "### Cargar datos y comprobar schema"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cars = spark.read.csv(DATA_PATH+'data/automobile.csv', sep=';', header=True, inferSchema=True)\n", "\n", "cars.printSchema()"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cars.show()"]}, {"metadata": {}, "source": ["\n", "\n", "### Vamos a trabajar con los valores nulos"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Una manera de devolver un dataframe sin filas que contengan nulos"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "cars.na.drop().show()"]}, {"metadata": {}, "source": ["\n", "\n", "Si quisieramos reemplazar los valores nulos con otro valor:"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "cars.na.fill(50).show()"]}, {"metadata": {}, "source": ["\n", "\n", "Otra forma de eliminar las filas con valores nulos (filtrar los nulos)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "import pyspark.sql.functions as F\n", "\n", "for column in cars.columns: \n", "    num_nulls = cars.where(F.col(column).isNull()).count()\n", "    \n", "    if num_nulls != 0:\n", "        cars = cars.where(F.col(column).isNotNull())\n", "        print(\"The column '{}' has {} nulls\".format(column,num_nulls))\n", "        print(\"The column '{}' has no more null values\".format(column))"]}, {"metadata": {}, "source": ["\n", "\n", "Cambiamos la variable objetivo para hacerla binaria, para poder utilizar algoritmos de clasificaci\u00f3n binaria. \n", "* -2, -1, 0 => no es muy seguro (0)\n", "* 1, 2, 3 => s\u00ed es seguro (1)"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql.types import DoubleType\n", "\n", "cars = cars.withColumn('symboling_binary', F.udf(lambda value: 0. if value <= 0 else 1., DoubleType())(F.col('symboling')))"]}, {"metadata": {}, "source": ["\n", "\n", "**Supongamos que queremos pasar la columna 'make' a dummy y luego lanzar un modelo de clasificaci\u00f3n.**\n", "\n", "**Resolviendo sin Pipeline** \n", "\n", "Tal como hemos visto antes, esto lo podr\u00edamos hacer paso a paso. \n", "\n", "1. Hacemos el StringIndexer a la columna _make_ para pasarla a num\u00e9rica (0... n_categorias-1)\n", "2. Hacemos el OneHotEncoder sobre el resultado del paso anterior para hacerla dummy\n", "3. Seleccionamos las variables que vamos a incluir en nuestro modelo (todas aquellas que no sean string y que no sean la variable objetivo _symboling_). Eso lo hacemos recorriendo `df.dtypes`, el cual nos devuelve una lista de tuplas, donde cada tupla tiene (nombre_variable, tipo_variable).\n", "4. Con las variables seleccionadas como predictoras del modelo, hacemos el VectorAssembler\n", "5. Dividimos train y test\n", "6. La salida del VectorAssembler ser\u00e1 lo que le demos al modelo (en este caso un Random Forest). Entrenamos (*fit*) y predecimos (*transform*)"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "string_indexer = StringIndexer(inputCol='make', outputCol='make_indexed')\n", "string_indexer_model = string_indexer.fit(cars)\n", "cars_many_steps = string_indexer_model.transform(cars)\n", "\n", "onehotencoder = OneHotEncoder(dropLast=False, inputCol= string_indexer.getOutputCol(), outputCol='make_encoded')\n", "cars_many_steps = onehotencoder.transform(cars_many_steps)\n", "\n", "columns_for_model = [element[0] for element in cars_many_steps.dtypes if element[1] != 'string' and 'symboling' not in element[0]]\n", "\n", "vector_assembler = VectorAssembler(inputCols=columns_for_model, outputCol='assembled_features')\n", "cars_many_steps = vector_assembler.transform(cars_many_steps)\n", "\n", "\n", "cars_many_steps_train, cars_many_steps_test = cars_many_steps.randomSplit([0.8,0.2])\n", "\n", "rf = RandomForestClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol='symboling_binary')\n", "rf_model = rf.fit(cars_many_steps_train)\n", "cars_many_steps = rf_model.transform(cars_many_steps_test)\n", "\n", "cars_many_steps.show()\n"]}, {"metadata": {}, "source": ["\n", "\n", "**Resolviendo con Pipeline** \n", "\n", "Es parecido a lo que hicimos sin pipeline, pero en lugar de hacer:\n", "- Crear el objeto string indexer, hacer *fit*, hacer *transform*\n", "- Crear el objeto OHE, hacer *transform*\n", "- Etc. \n", "\n", "Lo que hacemos es simplemente crear los objetos, los metemos como *stages* del pipeline, y luego le hacemos *fit* y *transform* al pipeline. Vamos a verlo."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml import Pipeline\n", "\n", "string_indexer = StringIndexer(inputCol='make', outputCol='make_indexed')\n", "\n", "onehotencoder = OneHotEncoder(dropLast=False, inputCol= string_indexer.getOutputCol(), outputCol='make_encoded')\n", "\n", "columns_for_model = [element[0] for element in cars.dtypes if element[1] != 'string' and 'symboling' not in element[0]] + [onehotencoder.getOutputCol()]\n", "vector_assembler = VectorAssembler(inputCols=columns_for_model, outputCol='assembled_features')\n", "\n", "rf = RandomForestClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol='symboling_binary')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "# Until here is the same as without pipeline but without doing fit / transform\n", "# We use this objects as stages for our pipeline\n", "\n", "pipeline = Pipeline(stages=[string_indexer, onehotencoder, vector_assembler, rf])\n", "\n", "cars_pipeline_train, cars_pipeline_test = cars.randomSplit([0.8,0.2])\n", "\n", "# We do fit and transform on the pipeline object\n", "\n", "pipeline_model = pipeline.fit(cars_pipeline_train)\n", "\n", "cars_pipeline = pipeline_model.transform(cars_pipeline_test)\n", "\n", "cars_pipeline.show()"]}, {"metadata": {}, "source": [" \n", "\n", "Los pipelines nos permiten reproducir todo el flujo cada vez que tenemos nuevos datos, de esta manera nos aseguramos de que cada nuevo \"batch\" se somete exactamente al mismo procesado.\n", "\n", "**Para guardar el pipeline completo:**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "pipeline_name = \"random_forest_pipeline\"\n", "models_path = DATA_PATH + \"models\" + \"_initials/\" # change the last part using your initials\n", "pipeline_model.save(models_path + pipeline_name)"]}, {"metadata": {}, "source": [" \n", "\n", "**Podemos despues cargar el pipeline de la siguiente manera**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml import PipelineModel"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "loaded_pipelinemodel = PipelineModel.load(models_path + pipeline_name)"]}, {"metadata": {}, "source": ["\n", "\n", "# Ejercicio 1\n", "\n", "Dado el siguiente DataFrame:"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql.functions import col as c\n", "from pyspark.sql import functions as F\n", "from pyspark.ml.feature import StandardScaler, MinMaxScaler\n", "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer, StringIndexerModel\n", "from pyspark.ml import Pipeline, PipelineModel\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "df = spark.read.csv(DATA_PATH + 'data/pokemon.csv', header=True, inferSchema=True)"]}, {"metadata": {}, "source": ["\n", "\n", "1) Realiza las transformaciones necesarias para comprobar que los datos son v\u00e1lidos para procesar (nombres de columnas, valores nulos).\n", "\n", "2) La variable que se va a predecir es una derivada de la columna `Speed`; cr\u00e9ala de forma que sea binaria, con 1 si el elemento supera la velocidad media del dataset, y 0 si no la supera.\n", "\n", "3) Define una funci\u00f3n que construya un `pipeline` con los pasos necesarios para preparar los datos y que aplique dicho pipeline a un DataFrame que reciba como par\u00e1metro de la funci\u00f3n.\n", "\n", "4) Construye un modelo de clasificaci\u00f3n usando `pyspark.ml.classification.RandomForestClassifier` para predecir si la velocidad de un pokemon es mayor a la media.\n", "\n", "5) Extrae la probabilidad de que la predicci\u00f3n sea 1 en una columna separada.\n", "\n", "6) Construye una funci\u00f3n que calcule los valores de *precision* y *recall* para diferentes valores de umbral (*threshold*), utilizando la definici\u00f3n (f\u00f3rmulas) de cada m\u00e9trica. Es decir, calcula el valor de las m\u00e9tricas para valores de umbral entre 0 y 1.\n", "\n", "**Extra**: Dibuja las curvas de *precision* y *recall* versus el umbral (eje x)."], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "**Parte 1**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "for col in df.columns:\n", "    df = df.withColumnRenamed(col, col.lower().replace('.', '').replace(' ', '_'))"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "print(df.count())\n", "df = df.dropna(how='any')\n", "print(df.count())"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 2**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "target = 'target'\n", "\n", "avg = df.agg(F.avg('speed').alias('speed')).first()['speed']\n", "\n", "df = df.withColumn('target', (c('speed') > avg).cast('double'))  # Both lines do the same\n", "df = df.withColumn('target', F.when(c('speed') > avg, F.lit(1.0)).otherwise(F.lit(0.0)))    # Both lines do the same\n", "\n", "df = df.drop('speed', 'sp_atk', 'sp_def', 'type_2')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "df.show(2)"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 3**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "def preprocess_to_model(df, targetCol, path, subset=None, scaling=None, mode=True):\n", "    if scaling not in (None, 'MinMax', 'Standard'):\n", "        raise ValueError(\"Scaling value error. Valid values are: None, MinMax, Standard.\")\n", "    \n", "    if subset is None:\n", "        subset = [col for col in df.columns if col != targetCol]\n", "    \n", "    df = df.select(subset + [targetCol])\n", "    \n", "    categorical_cols = [col for col, type_ in df.dtypes \n", "                        if type_ == 'string'\n", "                        and col != targetCol]\n", "    boolean_cols = [col for col, type_ in df.dtypes \n", "                    if type_ == 'boolean'\n", "                    and col != targetCol]\n", "    numerical_cols = [col for col in df.columns \n", "                      if col not in categorical_cols\n", "                      and col not in boolean_cols\n", "                      and col != targetCol]\n", "    \n", "    str_idxs = [StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='skip')\n", "                for col in categorical_cols] # handleInvalid='keep'\n", "    \n", "    ohes = [OneHotEncoder(inputCol=col+'_idx', outputCol=col+'_ohe')\n", "            for col in categorical_cols]\n", "    \n", "    if scaling is not None:\n", "        pre_scaling_assembler = VectorAssembler(inputCols=numerical_cols, \n", "                                                outputCol='numerical_features')\n", "        \n", "        if scaling == 'Standard':\n", "            scaler = StandardScaler(withMean=True, withStd=True, \n", "                                    inputCol='numerical_features', \n", "                                    outputCol='numerical_features_scaled')\n", "        else:\n", "            scaler = MinMaxScaler(inputCol='numerical_features', \n", "                                    outputCol='numerical_features_scaled')\n", "            \n", "        assembler = VectorAssembler(inputCols=[col+'_ohe' for col in categorical_cols] + \\\n", "                                              ['numerical_features_scaled'] + \\\n", "                                              boolean_cols, \n", "                                outputCol='features')\n", "        \n", "        stages = str_idxs+ohes+[pre_scaling_assembler,scaler, assembler]\n", "        \n", "    else:\n", "        assembler = VectorAssembler(inputCols=[col+'_ohe' for col in categorical_cols] + \\\n", "                                              numerical_cols + boolean_cols, \n", "                                outputCol='features')\n", "        \n", "        stages = str_idxs+ohes+[assembler]\n", "    \n", "    if mode:\n", "        pipeline = Pipeline(stages=stages).fit(df)\n", "        pipeline.save(path)\n", "        print('Saving pipeline object on ' + path)\n", "    else:\n", "        pipeline = PipelineModel.load(path)\n", "        print('Loading pipeline object from ' + path)\n", "    \n", "    df = pipeline.transform(df)\n", "    \n", "    return df.select('features', targetCol)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "df_train, df_test = df.randomSplit([0.8, 0.2])"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "subset = [col for col in df_train.columns if col != 'name']\n", "\n", "path = DATA_PATH + 'models' + '_initials/' + '/preprocess_to_model/' # change here \"_initials\" for your own initials\n", "\n", "df_train_preprocesed = preprocess_to_model(df_train, target, path, subset=subset,\n", "                                           scaling=None, mode=True)\n", "\n", "df_test_preprocesed = preprocess_to_model(df_test, target, path, subset=subset,\n", "                                          scaling=None, mode=False)\n", "\n", "df_train_preprocesed.cache()\n", "df_test_preprocesed.cache()"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 4**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "forest_classifier = RandomForestClassifier(featuresCol='features', labelCol=target)\n", "forest_classifier = forest_classifier.fit(df_train_preprocesed)\n", "df_forest_predicted = forest_classifier.transform(df_test_preprocesed)\n", "\n", "df_forest_predicted.show(5)"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 5**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql.types import ArrayType, DoubleType\n", "\n", "vector_to_array_udf = F.udf(lambda x: x.toArray().tolist(), ArrayType(DoubleType()))\n", "\n", "df_forest_predicted = df_forest_predicted.withColumn('probability',\n", "                                                         vector_to_array_udf(c('probability')))\n", "df_forest_predicted = df_forest_predicted.withColumn('probability',  c('probability')[1])\n", "df_forest_predicted.show(3)"]}, {"metadata": {}, "source": ["\n", "\n", "**Parte 6**"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "import numpy as np\n", "\n", "def calculate_metrics_by_threshold(df, pred_col, target, n_thresholds=99):\n", "    df = df.select(pred_col, target)\n", "    \n", "    # explode by threshold\n", "    thresholds = [np.round(x, 2) for x in np.linspace(0.01, 0.99, n_thresholds)]\n", "    \n", "    df = df.withColumn('threshold', F.array([F.lit(threshold) for threshold in thresholds]))\n", "    df = df.withColumn('threshold', F.explode(c('threshold'))) # create one row per each threshold associated with each user (n_thresholds times the same user)\n", "    \n", "    # calculating metrics\n", "    df = df.withColumn('PP', c(pred_col) > c('threshold')) ## PP = Predicted Positive \n", "    df = df.withColumn('TP', c('PP') & c(target).cast('boolean')) ## TP = True Positives \n", "    \n", "    agg_df = df.groupBy('threshold').agg(F.sum(c('PP').cast('int')).alias('PP'),\n", "                                         F.sum(c('TP').cast('int')).alias('TP'),\n", "                                         F.sum(c(target).cast('int')).alias('RP'))\n", "    agg_df = agg_df.withColumn('precision', c('TP')/c('PP'))\n", "    agg_df = agg_df.withColumn('recall', c('TP')/c('RP'))\n", "    agg_df = agg_df.withColumn('f1_score', 2 * (c('precision') * c('recall')) / (c('precision') + c('recall')))\n", "\n", "    agg_df = agg_df.select('threshold', 'precision', 'recall', 'f1_score')\n", "        \n", "    return agg_df"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "df.select(c(target).cast('boolean'))"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "by_threshold = calculate_metrics_by_threshold(df_forest_predicted, \n", "                                              'probability', target, n_thresholds=99)\n", "by_threshold"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "by_threshold = by_threshold.orderBy('threshold').toPandas()\n", "by_threshold"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from matplotlib import pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "by_threshold.plot(x='threshold', y=['precision', 'recall', 'f1_score'], figsize=(15,7))\n", "plt.show()"]}], "nbformat": 4, "nbformat_minor": 2}