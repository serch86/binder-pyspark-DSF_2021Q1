{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.7.3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": false, "toc_section_display": true, "base_numbering": 1, "toc_window_display": true, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {"height": "calc(100% - 180px)", "top": "150px", "left": "10px", "width": "273px"}, "nav_menu": {}}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Global data variables\n", "SANDBOX_NAME = # Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\" + SANDBOX_NAME + \"/data/data/\" "]}, {"metadata": {}, "source": ["\n", "\n", "# Combinando DataFrames\n", "\n", "En _pyspark_ hay dos formas de combinar los datos de DataFrames.\n", "* por filas: `join`\n", "* por columnas: `union`"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies_df = spark.read.csv(DATA_PATH + 'movie-ratings/movies.csv', sep=',', header=True, inferSchema=True)\n", "ratings_df = spark.read.csv(DATA_PATH + 'movie-ratings/ratings.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies_df.show(5)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_df.show(5)"]}, {"metadata": {}, "source": ["\n", "\n", "## join\n", "\n", "A\u00f1adamos a cada rating el t\u00edtulo y el g\u00e9nero de la pelicula."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_df.join(movies_df, on='movieId', how='inner')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.show(5)"]}, {"metadata": {}, "source": [" \n", "\n", "Si la columna de uni\u00f3n tuviera distinto nombre en ambos DataFrames"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies2_df = movies_df.withColumnRenamed('movieId', 'id_movie')\n", "movies2_df.show(2)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies2_df = ratings_df.join(movies2_df, \n", "                                     on=[ratings_df['movieId'] == movies2_df['id_movie']], how='outer')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies2_df.show(5)"]}, {"metadata": {}, "source": ["\n", "\n", "## union\n", "\n", "Imagina que tuvieramos un DataFrame con las pel\u00edculas de terror y otro con las de comedia y quisieramos unirlo todo en uno."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql import functions as F"]}, {"metadata": {}, "source": ["\n", "\n", "Crea un dataframe que contenga solamente aquellas pel\u00edculas que sean del g\u00e9nero `Horror` y otro que contenga aquellas del g\u00e9nero `Comedy`."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["horror_df = movies_df.filter(F.col('genres') == 'Horror')\n", "comedy_df = movies_df.filter(F.col('genres') == 'Comedy')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["horror_df.select('genres').distinct().show()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["comedy_df.select('genres').distinct().show()"]}, {"metadata": {}, "source": ["\n", "\n", "Mediante `union()` se pueden unir ambos dataframes. Comprueba que en el nuevo dataframe generado existen ambos g\u00e9neros."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["horror_comedy_df = horror_df.union(comedy_df)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["horror_comedy_df.select('genres').distinct().show()"]}, {"metadata": {}, "source": ["\n", "\n", "# Persistiendo DataFrames\n", "\n", "Debido al concepto de *lazy_evaluation* de Spark cada vez que realicemos una acci\u00f3n sobre el DataFrame `ratings_movies_df` se ejecutara la operaci\u00f3n de _join_. Este tipo de operaci\u00f3n es muy costosa computacionalmente por lo que es recomedable realizar un `cache` o `persist` sobre el DataFrame para evitar ejecutarla multiples veces.\n", "\n", "Al persistir un DataFrame se guarda temporalmente el resultado del DAG hasta el punto donde se cachea el DataFrame, evitando que se ejecute esa parte repetidas veces con cada acci\u00f3n."], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Por ejemplo, si queremos contar el n\u00famero de t\u00edtulos \u00fanicos con rating de 5 y tambi\u00e9n con rating de 1:\n", "\n", "__ineficiente:__"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "ratings_movies_df.filter(F.col('rating') == 5).select('title').distinct().count()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "ratings_movies_df.filter(F.col('rating') == 1).select('title').distinct().count()"]}, {"metadata": {}, "source": ["\n", "\n", "__eficiente__\n", "\n", "Observa que la primera acci\u00f3n puede ser incluso m\u00e1s lenta que la anterior ya que se est\u00e1 guardando el resultado. En cambio la segunda acci\u00f3n es mucho m\u00e1s r\u00e1pida al no necesitar volver a ejecutar el join."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.persist()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "ratings_movies_df.filter(F.col('rating') == 5).select('title').distinct().count()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "ratings_movies_df.filter(F.col('rating') == 1).select('title').distinct().count()"]}, {"metadata": {}, "source": ["\n", "\n", "Es importante borrar los DataFrames cacheados cuando no se vuelven a utilizar."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.unpersist()"]}, {"metadata": {}, "source": ["\n", "\n", "Es posible elegir si el guardado temporal se hace en memoria, en disco, o en ambas."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark import StorageLevel"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.persist(storageLevel=StorageLevel.DISK_ONLY)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_df.unpersist()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_df.unpersist()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.persist(storageLevel=StorageLevel.MEMORY_ONLY)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_df.unpersist()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["spark.catalog.clearCache()"]}, {"metadata": {}, "source": [" \n", "\n", "**Resumen de formas de persistencia**\n", "\n", "_Si queremos persistir un dataframe en memoria:_ `df.cache()`\n", "\n", "_Si queremos persistir un dataframe en memoria, en disco o en mixto:_ `df.persist(storageLevel=StorageLevel.<MEMORY_ONLY/DISK_ONLY/MEMORY_AND_DISK>)`\n", "\n", "_Si queremos borrar todos los dataframes que hemos cacheado con `cache()`:_ `spark.catalog.clearCache()`\n", "\n", "_Si queremos despersistir un dataframe que hemos persistido con `persist()`:_ `df.unpersist()`"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "# Ejercicio 1"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Usando los siguientes DataFrames:"], "cell_type": "markdown"}, {"metadata": {"colab": {}, "id": "KPxH3MjXY1Fu", "colab_type": "code"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies_df = spark.read.csv(DATA_PATH + 'movie-ratings/movies.csv', sep=',', header=True, inferSchema=True)\n", "ratings_df = spark.read.csv(DATA_PATH + 'movie-ratings/ratings.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {"id": "ptkHF10hY1F3", "colab_type": "text"}, "source": ["\n", "\n", "1) Crea un nuevo DataFrame que calcule el rating medio, m\u00e1ximo, y m\u00ednimo de cada pel\u00edcula.\n", "\n", "2) Filtra `movies_df` por las peliculas que contengan la palabra 'the' en el t\u00edtulo.\n", "\n", "3) Filtra `movies_df` por aquellas peliculas con al menos un `score` de 2 y una media de 4 o m\u00e1s. Usa un `left-semi` join.\n", "\n", "**Nota**: Los m\u00e9todos *left-semi* y *left-anti* joins son operaciones m\u00e1s r\u00e1pidas que los joins normales puesto que solo requieren reordenar la tabla de la derecha por clave primaria.\n", "\n", "4) Guarda en una lista el t\u00edtulo de todas las peliculas que contiene el DataFrame obtenido en el punto anterior."], "cell_type": "markdown"}, {"metadata": {"colab": {"height": 173, "base_uri": "https://localhost:8080/"}, "id": "8Kk2aoKUY1F7", "colab_type": "code", "outputId": "a97d8439-ab94-4fde-9bbd-06b4540a574c"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql import functions as F\n", "from pyspark.sql.functions import col\n", "\n", "agg_ratings = ratings_df.groupBy('movieId').agg(F.max(col('rating')).alias('max_rating'),\n", "                                  F.min(col('rating')).alias('min_rating'),\n", "                                  F.round(F.avg(col('rating')), 2).alias('avg_rating'))\n", "agg_ratings.show(3)"]}, {"metadata": {"colab": {"height": 489, "base_uri": "https://localhost:8080/"}, "id": "cmV39XffkEJD", "colab_type": "code", "outputId": "9054d967-6395-46d4-9a91-9875b12484f6"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "movies_df = movies_df.filter(col('title').rlike('the '))\n", "\n", "# If we want to create a perfect filter, we shall meake use of regex (which is not part of the course)\n", "movies_df.show(20, truncate=False)"]}, {"metadata": {"colab": {"height": 173, "base_uri": "https://localhost:8080/"}, "id": "MG4al3JnlCp6", "colab_type": "code", "outputId": "fa228512-c299-49c6-f429-0e0dd9700684"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "rating_filter_df = agg_ratings.filter(col('min_rating') >= 2).filter(col('avg_rating') >= 4)\n", "rating_filter_df.show(3)"]}, {"metadata": {"colab": {"height": 52, "base_uri": "https://localhost:8080/"}, "id": "xYOBA0ZXl4J0", "colab_type": "code", "outputId": "555dbc6c-b130-431c-9b32-f21329461445"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "print('movies_df number of rows: {}'.format(movies_df.count()))\n", "movies_filtered = movies_df.join(rating_filter_df, on=['movieId'], how='left_semi')\n", "print('movies_filtered number of rows: {}'.format(movies_filtered.count()))"]}, {"metadata": {"colab": {"height": 173, "base_uri": "https://localhost:8080/"}, "id": "lQCFvSpImGvM", "colab_type": "code", "outputId": "0c4c0aa9-cbff-4739-de2c-38b4614ef542"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "movies_filtered.show(3)"]}, {"metadata": {"colab": {"height": 208, "base_uri": "https://localhost:8080/"}, "id": "a-Z5h2BMnZiR", "colab_type": "code", "outputId": "e83b9a38-9157-4f41-b948-6683b347defb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "titles = [row[0] for row in movies_filtered.select('title').collect()]\n", "print(len(titles))\n", "titles[:10]"]}], "nbformat": 4, "nbformat_minor": 2}