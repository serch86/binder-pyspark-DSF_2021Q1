{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.5.2"}, "kernelspec": {"display_name": "d1-spark2python3", "language": "python", "name": "spark-python-d1-spark2python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": false, "toc_section_display": true, "base_numbering": 1, "toc_window_display": true, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {}, "nav_menu": {}}}, "cells": [{"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Global data variables\n", "SANDBOX_NAME = ''# Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\" + SANDBOX_NAME + \"/data/data/\" "]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql import functions as F"]}, {"metadata": {}, "source": ["\n", "\n", "# Creaci\u00f3n o modificaci\u00f3n de columnas\n", "\n", "En Spark hay un \u00fanico m\u00e9todo para la creaci\u00f3n o modificaci\u00f3n de columnas y es `withColumn`. Este m\u00e9todo es de nuevo una transformaci\u00f3n y toma dos par\u00e1metros: el nombre de la columna a crear (o sobreescribir) y la operaci\u00f3n que crea la nueva columna.\n", "\n", "Para una ejecuci\u00f3n m\u00e1s \u00f3ptima se recomienda utilizar \u00fanicamente las funciones de PySpark cuando se define la operaci\u00f3n, pero como se detallar\u00e1 m\u00e1s adelante se pueden utilizar funciones propias. "], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies_df = spark.read.csv(DATA_PATH + 'movie-ratings/movies.csv', sep=',', header=True, inferSchema=True)\n", "ratings_df = spark.read.csv(DATA_PATH + 'movie-ratings/ratings.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_df.join(movies_df, on='movieId', how='inner')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.cache()"]}, {"metadata": {}, "source": ["\n", "\n", "## Funciones de Spark"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "__valor fijo__\n", "\n", "El ejemplo m\u00e1s sencillo es crear una columna con un valor fijo, en este caso, columna `now` con valor '2019/01/21 14:08', y columna `rating2`con valor 4.0.\n", "\n", "Hint: `withColumn`"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('now', F.lit('2019/01/21 14:08'))"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.show(3)"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('rating2', F.lit(4.0))"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.show(3)"]}, {"metadata": {}, "source": ["\n", "\n", "__duplicar columna__"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('title2', F.col('title'))\\\n", "                 .select('title', 'title2')\\\n", "                 .show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "__operaciones aritmeticas__"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('rating_10', F.col('rating') * 2)\\\n", "                 .select('rating', 'rating_10')\\\n", "                 .show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('rating_avg', (F.col('rating') + F.col('rating2')) /  2)\\\n", "                 .select('rating', 'rating2', 'rating_avg')\\\n", "                 .show(10)"]}, {"metadata": {}, "source": [" \n", "\n", "__if/else__\n", "\n", "Crea la columna `kind_rating`, que sea 'high' en caso de que rating sea mayor que 4, y 'low' en caso contrario."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('kind_rating', \n", "                              F.when(F.col('rating') >= 4, 'high').otherwise('low')).show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "Se pueden concatenar multiples sentencias _when_. Esta vez, sobreescribe la columna `kind_rating` para crear un nivel intermedio, donde si es mayor que dos y menor que 4, `kind_rating` sea 'med'."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('kind_rating', \n", "                              F.when(F.col('rating') >= 4, 'high')\\\n", "                               .when(F.col('rating') >= 2, 'med')\\\n", "                               .otherwise('low')).show(20)"]}, {"metadata": {}, "source": ["\n", "\n", "__operaciones con strings__\n", "\n", "Pon en may\u00fasculas todos los t\u00edtulos de las pel\u00edculas"], "cell_type": "markdown"}, {"metadata": {"scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('title', F.upper(F.col('title'))).show(3)"]}, {"metadata": {}, "source": ["\n", "\n", "Extrae los 10 primeros caracteres de la columna `title`"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('short_title', F.substring(F.col('title'), 0, 10))\\\n", "                 .select('title', 'short_title')\\\n", "                 .show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "Separa los diferentes g\u00e9neros de la columna `genres` para obtener una lista, usando el separador '|'"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('genres', F.split(F.col('genres'), '\\|')).show(4)"]}, {"metadata": {}, "source": ["\n", "\n", "Crea una nueva columna `1st_genre` seleccionando el primer elemento de la lista del c\u00f3digo anterior"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('1st_genre', F.split(F.col('genres'), '\\|')[0])\\\n", "                 .select('genres', '1st_genre')\\\n", "                 .show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "Reemplaza el caracter '|' por '-' en la columna `genres`"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('genres', F.regexp_replace(F.col('genres'), '\\|', '-'))\\\n", "                 .select('title', 'genres')\\\n", "                 .show(10, truncate=False)"]}, {"metadata": {}, "source": ["\n", "\n", "_Con expresiones regulares_\n", "\n", "https://regexr.com/"], "cell_type": "markdown"}, {"metadata": {"scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('title', F.regexp_replace(F.col('title'), ' \\(\\d{4}\\)', '')).show(5, truncate=False)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('year', \n", "                                                 F.regexp_extract(F.col('title'),  '\\((\\d{4})\\)', 1))\n", "\n", "ratings_movies_df.show(5)"]}, {"metadata": {}, "source": ["\n", "\n", "## Casting\n", "\n", "Con el m\u00e9todo `withColumn` tambi\u00e9n es posible convertir el tipo de una columna con la funci\u00f3n `cast`. Es importante saber que en caso de no poder convertirse (por ejemplo una letra a n\u00famero) no saltar\u00e1 error y el resultado ser\u00e1 un valor nulo."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.printSchema()"]}, {"metadata": {}, "source": ["\n", "\n", "Cambia el formato de `year` a entero, y `movieId` a string."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('year', F.col('year').cast('int'))\n", "ratings_movies_df.show(5)"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('movieId', F.col('movieId').cast('string'))"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.printSchema()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('error', F.col('title').cast('int')).show(5)"]}, {"metadata": {}, "source": ["\n", "\n", "## UDF (User Defined Functions)\n", "\n", "Cuando no es posible definir la operaci\u00f3n con las funciones de spark se pueden crear funciones propias usando la UDFs. Primero se crea  una funci\u00f3n de Python normal y posteriormente se crea la UDFs. Es necesario indicar el tipo de la columna de salida en la UDF."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql.types import StringType, IntegerType, DoubleType, DateType"]}, {"metadata": {}, "source": ["\n", "\n", "_Aumenta el rating en un 15% para cada pel\u00edcula m\u00e1s antigua que 2000 (el m\u00e1ximo siempre es 5)._"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def increase_rating(year, rating):\n", "    \n", "    if year < 2000:\n", "        rating = min(rating * 1.15, 5.0)\n", "    \n", "    return rating"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["increase_rating_udf = F.udf(increase_rating, DoubleType())"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('rating_inc', \n", "                              increase_rating_udf(F.col('year'), F.col('rating')))\\\n", "                 .select('title', 'year', 'rating', 'rating_inc')\\\n", "                 .show(20)"]}, {"metadata": {}, "source": ["\n", "\n", "Extrae el a\u00f1o de la pel\u00edcula sin usar expresiones regulares."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["title = 'Trainspotting (1996)'"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["title.replace(')', '').replace('(', '')"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["year = title.replace(')', '').replace('(', '').split(' ')[-1]\n", "year = int(year)\n", "year"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def get_year(title): \n", "    \n", "    year = title.replace(')', '').replace('(', '').split(' ')[-1]\n", "    if year.isnumeric():\n", "        year = int(year)\n", "    else:\n", "        year = -1\n", "    \n", "    return year"]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["get_year_udf = F.udf(get_year, IntegerType())"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('year2', get_year_udf(F.col('title')))\\\n", "                 .select('title', 'year', 'year2').show(10, truncate=False)"]}, {"metadata": {}, "source": ["\n", "\n", "# Datetimes\n", "\n", "Hay varias funciones de _pyspark_ que permiten trabajar con fechas: diferencia entre fechas, dia de la semana, a\u00f1o... Pero para ello primero es necesario transformar las columnas a tipo fecha. Se permite la conversion de dos formatos de fecha:\n", "* timestamp de unix: una columna de tipo entero con los segundos trascurridos entre la medianoche del 1 de Enero de 1990 hasta la fecha.\n", "* cadena: la fecha representada como una cadena siguiendo un formato espec\u00edfico que puede variar."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('title', 'timestamp', 'now').show(5)"]}, {"metadata": {}, "source": [" \n", "\n", "## unix timestamp a datetime"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('datetime', F.from_unixtime(F.col('timestamp')))\n", "ratings_movies_df.select('datetime', 'timestamp').show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "## string a datetime"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df = ratings_movies_df.withColumn('now_datetime', \n", "                                                 F.from_unixtime(F.unix_timestamp(F.col('now'), 'yyyy/MM/dd HH:mm')))\n", "\n", "ratings_movies_df.select('now', 'now_datetime').show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "## funciones con datetimes"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('now_datetime', 'datetime', \n", "                          F.datediff(F.col('now_datetime'), F.col('datetime'))).show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('datetime', F.date_add(F.col('datetime'), 10)).show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.withColumn('datetime_plus_4_months', F.add_months(F.col('datetime'), 4))\\\n", "                  .select('datetime', 'datetime_plus_4_months').show(5)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('datetime', F.month(F.col('datetime')).alias('month')).show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('datetime', F.last_day(F.col('datetime')).alias('last_day')).show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.select('datetime', F.dayofmonth(F.col('datetime')).alias('day'),\n", "                                     F.dayofyear(F.col('datetime')).alias('year_day'),\n", "                                     F.date_format(F.col('datetime'), 'E').alias('weekday')).show(10)"]}, {"metadata": {}, "source": ["\n", "\n", "Para filtrar por fechas se pueden comparar directamente con una cadena en el formato YYYY-MM-DD hh:mm:ss ya que ser\u00e1 interpretada como una fecha."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.filter(F.col('datetime') >= \"2015-09-30 20:00:00\").select('datetime', 'title', 'rating').show(10)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.filter(F.col('datetime').between(\"2003-01-31\", \"2003-02-10\"))\\\n", "                  .select('datetime', 'title', 'rating').show(5)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ratings_movies_df.filter(F.year(F.col('datetime')) >= 2012)\\\n", "                 .select('datetime', 'title', 'rating').show(5)"]}, {"metadata": {"id": "RQQqo7LCY1GE", "colab_type": "text"}, "source": ["\n", "\n", "# Ejercicio 1\n", "\n", "1) Cree una funci\u00f3n que acepte un DataFrame y un diccionario. La funci\u00f3n debe usar el diccionario para renombrar un grupo de columnas y devolver el DataFrame ya modificado.\n", "\n", "Use el siguiente DataFrame y diccionario:"], "cell_type": "markdown"}, {"metadata": {"colab": {}, "id": "OZcBXSoEY1GG", "colab_type": "code"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["pokemon_df = spark.read.csv(DATA_PATH + 'pokemon.csv', sep=',', header=True, inferSchema=True)\n", "\n", "rename_dict = {'Sp. Atk': 'sp_atk',\n", "               'Sp. Def': 'sp_def'}"]}, {"metadata": {"colab": {"height": 173, "base_uri": "https://localhost:8080/"}, "id": "_gFkYDfbodna", "colab_type": "code", "outputId": "6ecc769f-077d-432e-99e4-c3bac2d74a7c"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["pokemon_df.show(3)"]}, {"metadata": {"colab": {}, "id": "bFegy2Nsogs7", "colab_type": "code"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "def rename_df(df, rename_dict):\n", "    if any(['.' in c for c in rename_dict.keys()]):\n", "    # withColumnRenamed method\n", "      for old, new in rename_dict.items():\n", "        df = df.withColumnRenamed(old, new)\n", "    else:\n", "      # Select method\n", "      df = df.select([col(c).alias(rename_dict.get(c, c)) for c in df.columns])\n", "    return df"]}, {"metadata": {"id": "Xy7_B4HbY1GL", "colab_type": "text"}, "source": ["\n", "\n", "2) Use la funci\u00f3n definida en el punto anterior para cambiar los nombres del DF usando el diccionario dado.\n", "\n", "3) Modifique la funci\u00f3n de tal forma que tambi\u00e9n acepte una funci\u00f3n en lugar de un diccionario. Use la funci\u00f3n para renombrar las columnas.\n", "\n", "4) Estandarice seg\u00fan las buenas pr\u00e1cticas los nombres de las columnas usando la funci\u00f3n que acaba de definir.\n", "\n", "5) Cree otra funci\u00f3n que acepte un DataFrame y una lista con un subconjunto de columnas. El objetivo de esta funci\u00f3n es determinar el n\u00famero de filas duplicadas del DF.\n", "\n", "6) Use la funci\u00f3n creada para obtener el n\u00famero de duplicados del DataFrame pokemon_df en todas las columnas excepto el nombre (`name`)"], "cell_type": "markdown"}, {"metadata": {"colab": {"height": 72, "base_uri": "https://localhost:8080/"}, "id": "f8VKWgPuY1GO", "colab_type": "code", "outputId": "dd44e9f9-14d9-4a18-b819-7eafd0a86f85"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "print(pokemon_df.columns)\n", "pokemon_df = rename_df(pokemon_df, rename_dict)\n", "print(pokemon_df.columns)"]}, {"metadata": {"colab": {}, "id": "QJlJcyBKqQf0", "colab_type": "code"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "def rename_df(df, rename_object):\n", "    if isinstance(rename_object, dict):\n", "        if any(['.' in c for c in rename_object.keys()]):\n", "           # withColumnRenamed method\n", "           for old, new in rename_object.items():\n", "                df = df.withColumnRenamed(old, new)\n", "        else:\n", "           # Select method\n", "           df = df.select([col(c).alias(rename_object.get(c, c)) for c in df.columns])\n", "    elif isinstance(rename_object, type(lambda x: x)):\n", "        for c in df.columns:\n", "            df = df.withColumnRenamed(c, rename_object(c))\n", "    else:\n", "        raise Exception('Not implemented')\n", "    return df"]}, {"metadata": {"colab": {"height": 52, "base_uri": "https://localhost:8080/"}, "id": "eDV-hsD_r5Yu", "colab_type": "code", "outputId": "6f03bb83-2962-482c-e28b-86f95b7a0184"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "print(pokemon_df.columns)\n", "pokemon_df = rename_df(pokemon_df, lambda c: c.strip().lower().replace('.', '').replace(' ', '_'))\n", "print(pokemon_df.columns)"]}, {"metadata": {"colab": {}, "id": "VHOh4-1Btc6O", "colab_type": "code"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "def show_duplicates(df, subset):\n", "    assert isinstance(subset, (list, tuple)), 'Subset is not a list neither a tuple'\n", "  \n", "    agg_count = df.groupBy(subset).count().filter(F.col('count') > 1)\n", "    df = df.join(agg_count, on=subset, how='inner')\n", "    return df\n", "\n", "# Note: this could be done more efficiently and with less potential bugs using a window function."]}, {"metadata": {"colab": {"height": 139, "base_uri": "https://localhost:8080/"}, "id": "mMji9U1gtdEG", "colab_type": "code", "outputId": "bd7ec08a-ca36-4f6f-cd03-a42017615803"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "subset = [c for c in pokemon_df.columns if c != 'name']\n", "\n", "show_duplicates(pokemon_df, subset).show()"]}, {"metadata": {}, "source": ["\n", "\n", "# Ejercicio 2\n", "\n", "Crea la misma l\u00f3gica definida en el siguiente UDF, pero sin usar UDFs, es decir, usando exclusivamente funciones de SparkSQL."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["movies_df = spark.read.csv(DATA_PATH + 'movie-ratings/movies.csv', sep=',', header=True, inferSchema=True)\n", "movies_df = movies_df.withColumn('genres', F.split(F.col('genres'), '\\|'))\n", "\n", "from pyspark.sql.types import StringType, IntegerType, DoubleType, BooleanType\n", "\n", "def value_in_col(col, value):\n", "    return value in col\n", "\n", "value_in_col_udf = F.udf(value_in_col, BooleanType())"]}, {"metadata": {}, "source": ["\n", "\n", "*Pista*: Mira la funci\u00f3n *explode*."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta\n", "\n", "def filter_by_array_value(df, array_col, value):\n", "    df = df.withColumn('element', F.explode(F.col(array_col)))\n", "    df = df.filter(F.col('element') == value)\n", "    df = df.drop('element')\n", "    return df\n", "  \n", "filter_by_array_value(movies_df, 'genres', 'Drama').show(10, truncate=False)"]}], "nbformat": 4, "nbformat_minor": 2}