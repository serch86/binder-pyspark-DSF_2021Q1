{"metadata": {"language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.5.2"}, "kernelspec": {"display_name": "d1-spark2python3", "language": "python", "name": "spark-python-d1-spark2python3"}, "toc": {"title_cell": "Table of Contents", "skip_h1_title": false, "number_sections": false, "toc_section_display": true, "base_numbering": 1, "toc_window_display": true, "toc_cell": false, "sideBar": true, "title_sidebar": "Contents", "toc_position": {}, "nav_menu": {}}}, "cells": [{"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Global data variables\n", "SANDBOX_NAME = ''# Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\" + SANDBOX_NAME + \"/data/data/\" "]}, {"metadata": {}, "source": ["\n", "\n", "# Valores Ausentes\n", "\n", "Los valores ausentes en _pyspark_ est\u00e1n identificados como _null_. El m\u00e9todo `isNull` permite idenficar los registros nulos y `isNotNull` los no nulos."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from pyspark.sql import functions as F"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df = spark.read.csv(DATA_PATH + 'crime_in_vancouver.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.filter(F.col('NEIGHBOURHOOD').isNull()).show(4)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.filter(F.col('NEIGHBOURHOOD').isNotNull()).show(4)"]}, {"metadata": {}, "source": [" \n", "\n", "## Conteo de valores nulos"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.filter(F.col('NEIGHBOURHOOD').isNull()).count()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.filter(F.col('TYPE').isNull()).count()"]}, {"metadata": {}, "source": ["\n", "\n", "### Porcentaje de ausentes por columna\n", "\n", "El primer m\u00e9todo es menos eficiente que el segundo ya que requiere ejecutar una acci\u00f3n por cada columna. Como norma general en Spark hay que intentar realizar el n\u00famero m\u00ednimo de acciones."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["n_rows_vancouver = vancouver_df.count()"]}, {"metadata": {}, "source": ["\n", "\n", "__M\u00e9todo 1:__"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "for col in vancouver_df.columns:\n", "    \n", "    n_missing = vancouver_df.filter(F.col(col).isNull()).count()\n", "    perc_missing = 100 * n_missing / n_rows_vancouver\n", "    \n", "    print(col, round(perc_missing, 2))"]}, {"metadata": {}, "source": ["\n", "\n", "__M\u00e9todo 2:__\n", "\n", "Para una \u00fanica columna"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.select(F.round(F.sum(F.col('NEIGHBOURHOOD').isNull().cast('int')) * 100 / n_rows_vancouver, 2)\\\n", "                      .alias('NEIGHBOURHOOD')).show()"]}, {"metadata": {}, "source": ["\n", "\n", "Todas las columnas"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time \n", "\n", "missing_ops = [F.round(F.sum(F.col(c).isNull().cast('int')) * 100 / n_rows_vancouver, 2).alias(c) \n", "               for c in vancouver_df.columns]\n", "\n", "vancouver_df.select(missing_ops).show()"]}, {"metadata": {}, "source": [" \n", "\n", "## Eliminaci\u00f3n registros nulos\n", "\n", "El m\u00e9todo `dropna` se utiliza para eliminar registros nulos. Con el par\u00e1metro `subset` se indican sobre qu\u00e9 columnas buscar nulos y el par\u00e1metro `how` selecciona con qu\u00e9 condici\u00f3n se elimina un registro. Por defecto, `how` est\u00e1 a 'any'."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.dropna(how='all').count()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.dropna(how='any').count()"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_no_missing_df = vancouver_df.dropna(subset=['HOUR', 'MINUTE'])"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_no_missing_df.select(missing_ops).show()"]}, {"metadata": {}, "source": ["\n", "\n", "## Imputaci\u00f3n de valores nulos\n", "\n", "`fillna` imputa los valores nulos de las columnas a un valor fijo elegido."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.show(3)"]}, {"metadata": {}, "source": ["\n", "\n", "Imputa los valores nulos de las columnas `HOUR` y `MINUTE` por el valor 0, y los de la columna `NEIGHBOURHOOD` por 'Unknown'."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.fillna(0, subset=['HOUR', 'MINUTE']).show(3)"]}, {"metadata": {"scrolled": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df.fillna('Unknown', subset=['NEIGHBOURHOOD']).show(3)"]}, {"metadata": {}, "source": ["\n", "\n", "## Ejercicio 1"], "cell_type": "markdown"}, {"metadata": {}, "source": ["\n", "\n", "Usando el siguiente dataframe"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["vancouver_df = spark.read.csv(DATA_PATH + 'crime_in_vancouver.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {}, "source": ["\n", "\n", "- a. Determine que columna(s) tiene(n) el mayor n\u00famero de nulos\n", "- b. Complete las variables categ\u00f3ricas con nulos con el valor mayoritario\n", "- c. Elimine los registros con mayor n\u00famero de nulos\n", "- d. Complete las variables cuantitativas con nulos con los valores medios correspondientes de esas columnas"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "## Ejercicio 2\n", "\n", "Fuente de los datos: https://www.kaggle.com/abhinav89/telecom-customer\n", "\n", "1) Obtener un diccionario de las variables con el valor del porcentaje de nulos que contengan. Ordenarlo, de alguna forma aunque la salida no sea un diccionario, de mayor a menor porcentaje de nulos.\n", "\n", "2) Realiza el tratamiento que consideres para los datos nulos, en funci\u00f3n del significado de negocio que consideres para cada caso y la cantidad de datos nulos que contenga la columna. Imputar al menos cinco columnas a modo de ejemplo, justificando los valores sustituidos a nivel de negocio.\n", "\n", "Hint: consideraremos que la columna no aporta valor si contiene m\u00e1s del 40% de sus valores nulos\n"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df = spark.read.csv(DATA_PATH + 'telecom_customer_churn.csv', sep=',', header=True, inferSchema=True)"]}, {"metadata": {"scrolled": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df.count()"]}, {"metadata": {}, "source": ["\n", "\n", "1) Obtener un diccionario de las variables con el valor del porcentaje de nulos que contengan. Ordenarlo, de alguna forma aunque la salida no sea un diccionario, de mayor a menor porcentaje de nulos."], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "source": ["\n", "\n", "2) Realiza el tratamiento que consideres para los datos nulos, en funci\u00f3n del significado de negocio que consideres para cada caso y la cantidad de datos nulos que contenga la columna. Imputar al menos cinco columnas a modo de ejemplo, justificando los valores sustituidos a nivel de negocio.\n", "\n", "Hint: consideraremos que la columna no aporta valor si contiene m\u00e1s del 40% de sus valores nulos"], "cell_type": "markdown"}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Respuesta aqui"]}], "nbformat": 4, "nbformat_minor": 2}